All Available Endpoints                                                                                                                                                                                 
                                                                                                                                                                                                          
Public (no auth needed)                                                                                                                                                                                 

curl http://localhost:8000/                                                                                                                                                                             
curl http://localhost:8000/health                       
curl http://localhost:8000/docs        # Swagger UI

User endpoints (need user API key)

# Chat completion
curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer <api_key>" -d '{"model": "llama3.2", "messages": [{"role": "user", "content": "Hello"}]}'

# Chat completion (streaming)
curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer <api_key>" -d '{"model": "llama3.2", "messages": [{"role": "user", "content": "Hello"}],
"stream": true}'

# List models
curl http://localhost:8000/v1/models -H "Authorization: Bearer <api_key>"

# Get your usage
curl http://localhost:8000/v1/usage -H "Authorization: Bearer <api_key>"

# Get usage summary by model
curl http://localhost:8000/v1/usage/summary -H "Authorization: Bearer <api_key>"

Admin endpoints (need admin key)

# Create user
curl -X POST http://localhost:8000/admin/users -H "Authorization: Bearer admin-secret-key" -H "Content-Type: application/json" -d '{"user_id": "user-123"}'

# List all users
curl http://localhost:8000/admin/users -H "Authorization: Bearer admin-secret-key"

# Get specific user
curl http://localhost:8000/admin/users/user-123 -H "Authorization: Bearer admin-secret-key"

# Delete user
curl -X DELETE http://localhost:8000/admin/users/user-123 -H "Authorization: Bearer admin-secret-key"

# Get user's usage
curl http://localhost:8000/admin/users/user-123/usage -H "Authorization: Bearer admin-secret-key"

# Get user's rate limits
curl http://localhost:8000/admin/users/user-123/limits -H "Authorization: Bearer admin-secret-key"

# Update user's rate limits
curl -X PUT http://localhost:8000/admin/users/user-123/limits -H "Authorization: Bearer admin-secret-key" -H "Content-Type: application/json" -d '{"requests_per_minute": 10, "tokens_per_day": 10000}'

---
On Performance

The slowness you're seeing is Ollama/the model, not the proxy. The proxy itself just passes data through — it adds maybe 1-2ms of overhead.

The bottleneck is always the LLM inference. With hundreds of concurrent users, the question is how fast Ollama can process requests, which depends on:
┌────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│       Factor       │                                                             Impact                                                             │
├────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ GPU                │ By far the biggest factor. A MacBook Air's integrated GPU is limited. A dedicated NVIDIA GPU (RTX 4090, A100) is 10-50x faster │
├────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ Model size         │ llama3.2 (3B) is much slower than llama3.2:1b. Smaller models = faster                                                         │
├────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ RAM                │ Models run in memory. More RAM = can serve more concurrent requests without swapping                                           │
├────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ Ollama concurrency │ Ollama processes requests sequentially by default. Multiple requests queue up                                                  │
└────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
For real multi-user scaling, the answer is yes — youd want:
- A machine with a dedicated GPU (or multiple)
- Or multiple Ollama instances behind a load balancer
- Or a cloud GPU service (AWS, RunPod, etc.)

The proxy server itself (FastAPI + async) can handle thousands of concurrent connections easily — it's just waiting on Ollama. Run your load test and you'll see the proxy stays responsive while Ollama
becomes the bottleneck.

Unit Tests (no Ollama needed)

pytest tests/test_basic.py -v

Integration Tests (requires Ollama + running server)

# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start the proxy server
python main.py

# Terminal 3: Run tests
SKIP_INTEGRATION_TESTS=false pytest tests/test_streaming.py tests/test_vision.py -v -s

OpenAI SDK test (streaming)

from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="<api_key>")
for chunk in client.chat.completions.create(
    model="llama3.2:1b",
    messages=[{"role": "user", "content": "Count to 5"}],
    stream=True
):
    print(chunk.choices[0].delta.content, end="")

Load Test

# Quick test
TEST_API_KEY=sk-user-123-9c249983d66d52e6e5f2c227bb5717ed python tests/test_load.py 
python tests/test_load.py

# Full load test with locust
locust -f tests/test_load.py --host=http://localhost:8000 --users=100 --spawn-rate=10